{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af6eb2-1339-474a-95be-9bdd232c80b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b2b78d-79d0-4acb-885e-db611214c6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2832592-8e6c-4e32-a79b-3dd0bcbb69b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e29f8ab-2ed0-4955-a32a-59733d49e073",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "model = nn.Sequential(OrderedDict([\n",
    "    #reduce size of image from (28*28*1) to (28*28*32) due to padding\n",
    "    ('conv1', nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)),\n",
    "    ('relu1', nn.ReLU()),\n",
    "    #reduce size of image from (28*28*32) to (28*28*64) due to padding\n",
    "    ('conv2', nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)),\n",
    "    ('relu2', nn.ReLU()),\n",
    "    #reduce size of image from (28*28*64) to (14*14*64)\n",
    "    ('maxpool1', nn.MaxPool2d(kernel_size=2, stride=2)),\n",
    "    #to make it 1D vector for the Linear layer\n",
    "    # The flatten layer converts the (14*14*64) tensor into a 1D vector of size 12544.\n",
    "    ('flatten', nn.Flatten()),\n",
    "    #input: 12544   output: 256\n",
    "    ('fc1', nn.Linear(64 * 14 * 14, 256)),\n",
    "    ('relu5', nn.ReLU()),\n",
    "    #input: 256   output: 10\n",
    "    ('fc2', nn.Linear(256, 10)),\n",
    "]))\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84210d43-9b82-43a0-996f-7115cac24235",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the network, define the criterion and optimizer\n",
    "from torch.optim import lr_scheduler\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr=learning_rate,)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f5c6f-e848-4115-80e1-1db601bc93db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the network here\n",
    "epochs = 20\n",
    "print_every = 40\n",
    "steps = 0\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images,labels in iter(trainloader):\n",
    "        steps += 1\n",
    "        # Clear gradients as they are accumulative\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        out = model(images)  # Use the model directly\n",
    "        # Loss calculation\n",
    "        loss = criterion(out, labels)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        #extra for printing the traing process and the loss\n",
    "        running_loss += loss.item()\n",
    "        if steps % print_every == 0:\n",
    "            print(\"Epoch: {}/{}... \".format(e+1, epochs),\n",
    "                  \"Loss: {:.4f}\".format(running_loss/print_every))\n",
    "            \n",
    "            running_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb79a4ea-04ce-44e9-ad2b-819ed8d077ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the network\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "img = images[0]\n",
    "# Convert 2D image to 1D vector\n",
    "img = img.view(1, 1, 28, 28)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(img)\n",
    "\n",
    "ps = F.softmax(logits, dim=1)\n",
    "\n",
    "# Plot the image and probabilities\n",
    "helper.view_classify(img.resize_(1, 28, 28), ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f91f9-98d0-457f-a22e-55f4963c791b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d17a71-af13-4ba6-8dfc-57d0e709a29a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
